---
layout: post
title: Deep Learning Fundamentals
subtitle: How to Build Deep Neural Networks
---

The following will be a collection of notes on deep learning. I plan to
cover the fundamentals, common network architectures, and recent
research advances.

Language of Instruction
=======================

Like in most areas of science, deep learning can be described in the
most concise and exact way using mathematics. Unfortunately, mathematics
is not a language that most humans, including myself, are commonly
familiar with. Therefore, as I write these notes I will try my best to
explain concepts using basic English before I describe them
mathematically. However, as we will soon find out, it will be very
difficult to ignore the math and explain everything with verbosity.

```python
def function(x) {
  return(x + 5);
}
function(3)
```

{: .box-note}
**Note:** This is a notification box.

The Problems We Try To Solve
============================

Before we dive straight into deep learning, it is important to think
about what they can be used for. For now we will focus on one type of
problems that deep learning tries to solve: supervised learning
problems. In supervised learning, we are given a data set of inputs and
their corresponding correct outputs. We use this data set to train our
deep learning model. Afterwards, we use our trained model to look at
previously unseen inputs and predict their correct outputs. An analogy
to supervised learning would be a student who prepares for his exam by
studying a bunch of solved exam questions. We assume that all possible
questions will come from the same book.

{: .box-note}
**Note:** The book analogy refers to the assumption that both the training
and test sets come from the same data-generating distribution.

Model Representation
====================

Let the input to the model be $X$. The matrix $X$ consists of $m$
training examples and each example $\mathbf{x}$ consists of $n_{x}$
features. The i-th training example is denoted as $\mathbf{x}^{i}$,
while the j-th feature of that training examples is denoted as
$x_{j}^{i}$. See the figure below for an illustration.

$$X = \begin{pmatrix}
x_{1}^{1} & \dots & x_{1}^{m} \\
\vdots    & \ddots & \vdots \\
x_{n_{x}}^{1} & \dots & x_{n_{x}}^{m}
\end{pmatrix}$$

The model has $L$ layers. Each layer contains a set of weights and
biases. Let the matrix $W^{l}$ represent the weights of the l-th layer
and the vector $\mathbf{b}^{l}$ represent the biases of the l-th layer.

Let the output to the model be $Y$. The matrix $Y$ consists of $m$
outputs, one for each training example $\mathbf{x}$. In this tutorial, 
each $\mathbf{y}$ will be a scalar (i.e. a single number). 

Initialization
==============

Before a model can be trained on a data set, the values of it’s weights
must be set to initial values. This is called weight initialization. It
might seem that setting all weight values to zero would be a simple
solution. However, this would essentially mean that every neuron in a
given layer $l$ would learn the same thing. This would mean that the
model would be no different than one having one hidden unit (neuron) in
each layer. Hence, we need to initialize the weights in a way that would
break symmetry in the layer’s units. This can be achieved by randomly
initializing the weights using some distribution. For the purposes of
our model, we choose an initialization method that works well for the
activation functions that we will use (ReLU). This method is known as He
initialization and uses a Gaussian distribution with zero mean and
variance of $$Var(W^{l}) = \frac{2}{n_{l}}$$ where $n_{l}$ denotes the
number of hidden units in layer $l$.

There are various other initialization methods used in deep learning and
it is currently an active area of research. Further discussion on
commonly used methods will be in another post.

Forward Propagation
===================

Forward propagation is the sequence of computations that a model
performs on an input to produce a prediction. Each layer takes the 
output from the previous layer and performs two main computation steps: 
- Linear combination
- Activation
The linear combination step can be expressed as
$$Z^{l} = W^{l}A^{l-1} + b^{l}$$
where $A^{l-1}$ denotes the activation
output of the previous layer.

{: .box-note}
**Note:** In the first layer, $A^{l-1}$ would represent the input $X$.

'''python
def linear_combination(A_prev, W, b):
    Z = W.dot(A_prev) + b
    return Z
''' 

The activation step can be expressed as 
$$A^{l} = g^{l}\odot(Z^{l})$$
where $g\odot()$ denotes an activation function that is applied 
element-wise to $Z^{l}$. There are several types of activation functions 
that can be used in deep learning. For this project, we use the ReLU 
activation function for the hidden layers, which can be expresses as
$$g(z_{j}^{[l](i)}) =   \begin{cases}
    0 & \text{if $z_{j}^{[l](i)} \leq 0$} \\
    1 & \text{otherwise}
  \end{cases}$$
where $z_{j}^{[l](i)}$ denotes an element in $Z^{[l]}$; and $g()$
denotes the ReLU activation function. Here is a Python implementation: 

'''python
def relu(Z):
    A = np.maximum(0,Z)
    return A
'''

In the final layer, we use the sigmoid activation function, which can be expressed as
$$g(z_{j}^{[L](i)}) = \frac{1}{1 + e^{-z_{j}^{[L](i)}}}$$
where $z_{j}^{[L](i)}$ is an element in $Z^{[L]} and $g()$ denotes the 
sigmoid activation function. Here is a Python implementation:

'''python
def sigmoid(Z):
    A = 1/(1+np.exp(-Z))
    return A
'''

Forward propagation involves performing these two computations 
successively through the model's hidden layers. The output of the final
layer is the model's prediction. Here is a Python implementation:

'''python
def forward_propagation(X, parameters):
    for l in range(1, L):
        # grab the weight matrix and bias vector of the layer
        W = parameters["W" + str(l)]
        b = parameters["b" + str(l)]
        A_prev = A 
        # compute linear combination 
        Z = linear_combination(A_prev, W, b)
        # cache values for later use in backprop
        cache_list.append((A_prev, W, b, Z))
        # compute non-linear activation
        A = activation(Z)

    # Output Layer Forward Propagation
    W = params["W" + str(L)]
    b = params["b" + str(L)]
    A_prev = A
    Z = linear_combination(A_prev, W, b)
    AL = final_activation(Z)
    cache_list.append((A_prev, W, b, Z))
            
    return AL, cache_list
'''

{: .box-note}
**Note:** Notice how we cache $A^{[l-1]}$, $W^{[l]}$, $\mathbf{b}^{[l]}$, 
and $Z^{[l]}$ values. This is because we will re-use them during 
back propagation.


Cost Function
=============

The cost function computes the error $E$ between the model’s prediction
$A^{L}$ and the true output $Y$. The error can be thought of as a
distance measure that describes how far the model is from perfectly
predicting the output. For our purposes we will use the binary
cross-entropy cost function, which is defined as
$$E(A^{l}, Y) = - \frac{1}{m} \sum \limits_{i=1}^{m} (y^{(i)} \log( a^{L(i)}) +$$

Backward Propagation
====================

Backward propagation is the sequence of computations that a model
performs to qunatitatively express the relationships between the model’s
error $E$ and the weights $W^{l}$ and biases $\mathbf{b^{l}}$ in each
layer $l$. It is these relationships that we later use to update the
model’s parameters. This is how the model learns from the training data.

In mathematics, we can describe the relationship between two variables
using differentiation. The derivative of a variable $y$ with respect to
$x$ describes the rate of change in $y$ as $x$ is changed. This can be
expressed as $$\frac{dy}{dx}$$

In our model, we need to express the derivative of the error with
respect to many other variables (activations, weights, biases...etc.).
For this we use partial differentiation where the partial derivative of
$y$ with respect to $x$ describes the rate of change in $y$ as $x$
changes, but all other variables are kept constant.

First, we express the partial derivative of $E$ with respect to the
model’s output
$$\frac{\partial E}{\partial A^{[L]}} =  \frac{Y}{A^{[L]}} - \frac{1-Y}{1-A^{[L]}}$$
where $Y$ is a matrix containing the correct outputs for each training
example.

Then, we need to go backwards and express the error with respect to the
linear output of the last layer $Z^{[L]}$. For this we use the chain
rule for partial differentiation as such
$$\frac{\partial E}{\partial Z^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial Z^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \sigma^{'}(Z^{[L]}) = A^{[L]} - Y$$
where $\sigma^{'}$ denotes the derivative of the sigmoid activation
function, which we use for our output layer, with respect to $Z^{L}$.

Similarly, we then keep propagating the error backwards to express it’s
relationship with the weights and biases of each layer in the model. The
expressions can be summarised as follows
$$\frac{\partial E}{\partial Z^{[l]}} =  \frac{\partial E}{\partial A^{[l]}} \odot g^{l'}(Z^{[l]})$$

$$\frac{\partial E}{\partial W^{[l]}} = \frac{1}{m} \frac{\partial E}{\partial Z^{[l]}} A^{ [l -1 ] T}$$

$$\frac{\partial E}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum \limits_{i=1}^{m}\frac{\partial E}{\partial Z^{[l]}}$$

$$\frac{\partial E}{\partial A^{[l-1]}} = W^{[l]T} \frac{\partial E}{\partial Z^{[l]}}$$

where $g^{[l]'}$ denotes the derivative of the activation function that
is applied element-wise in layer $l$; $A^{[l-1]T}$ denotes the transpose
of matrix $A^{[l-1]}$; and $m$ is the number of training examples.

Updating Model Parameters
=========================

After backpropagation, we are left with a set of relationships between
the model’s error and it’s parameters. The next step is to use them to
update the values of the parameters in such a way so as to reduce the
error. These updates are carried out using optimizers. Stochastic
gradient descent (SGD) is a very common optimizer in deep learning. The
updates that SGD does on the parameters can be expressed as
$$W^{[l]}_{new} = W^{[l]}_{old} - \alpha \frac{\partial E}{\partial W^{[l]}_{old}}$$

$$\mathbf{b}^{[l]}_{new} = \mathbf{b}^{[l]}_{old} - \alpha \frac{\partial E}{\partial \mathbf{b}^{[l]}_{old}}$$

where $\alpha$ is the learning rate, which is a value that describes how
fast (or slow) the model learns from the training examples.

Training Our Model
==================

That’s pretty much it. The general procedure behind deep learning
involves the following steps:

-   Choose a suitable model architecture, number of hidden layers, and
    types of activation functions

-   Initialize the model parameters (weights and biases)

-   Feed the input into the model

-   Compute the model prediction using forward propagation

-   Compute the error using the cost function

-   Compute the partial derivatives of the error with respect to the
    parameters using back propagation

-   Update the model parameters using an optimizer

-   Repeat until satisfied

Seems simple enough, right? Well, there are various methods to carry out
each step and finding just the right combination of methods can be
rather elusive, especially for solving complicated problems.

In this final section, we will use all of the different components we
built in the previous sections to create a training script for our
model. But before that, we need to prepare our data to make training
effective. Basically, we will divide our training set into random and
distinct chunks called mini-batches. During each iteration, we will feed
a mini-batch as input to the model for training. After feeding all the
mini-batches from the training set, the model has been trained on all
the training examples. This is called an epoch. We then repeat model
training to complete more epochs until the model’s accuracy reaches a
sufficiently high value.

We first need to create a function that takes a training set and returns
a set of random and distinct mini-batches.

We can then commence training our model. We build a script that takes
the training data and necessary initial values for the model, and then
carry out model training using the general procedure described above.
